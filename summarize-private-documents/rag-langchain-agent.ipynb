{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce00dbbb",
   "metadata": {},
   "source": [
    "<!-- #### SUMMARIZE PRIVATE DOCUMENTS using AGENT - RAG, LANGCHAIN , LLM   -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136a904b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain==0.1.16\n",
      "  Using cached langchain-0.1.16-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/user/codex/langchain/.venv/lib64/python3.13/site-packages (from langchain==0.1.16) (6.0.3)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain==0.1.16)\n",
      "  Using cached sqlalchemy-2.0.45-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain==0.1.16)\n",
      "  Using cached aiohttp-3.13.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.1.16)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.1.16)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.32 (from langchain==0.1.16)\n",
      "  Using cached langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting langchain-core<0.2.0,>=0.1.42 (from langchain==0.1.16)\n",
      "  Using cached langchain_core-0.1.53-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain==0.1.16)\n",
      "  Using cached langchain_text_splitters-0.0.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.1.16)\n",
      "  Using cached langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy<2,>=1 (from langchain==0.1.16)\n",
      "  Using cached numpy-1.26.4.tar.gz (15.8 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /home/user/codex/langchain/.venv/lib64/python3.13/site-packages (from langchain==0.1.16) (2.12.5)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/user/codex/langchain/.venv/lib64/python3.13/site-packages (from langchain==0.1.16) (2.32.5)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain==0.1.16)\n",
      "  Using cached tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/user/codex/langchain/.venv/lib64/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (25.4.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16)\n",
      "  Downloading frozenlist-1.8.0-cp313-cp313-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16)\n",
      "  Downloading multidict-6.7.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16)\n",
      "  Downloading propcache-0.4.1-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16)\n",
      "  Downloading yarl-1.22.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.16)\n",
      "  Using cached marshmallow-3.26.2-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.16)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain==0.1.16)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.42->langchain==0.1.16)\n",
      "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/user/codex/langchain/.venv/lib64/python3.13/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/user/codex/langchain/.venv/lib64/python3.13/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (3.11.5)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain==0.1.16)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: anyio in /home/user/codex/langchain/.venv/lib64/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (4.12.0)\n",
      "Requirement already satisfied: certifi in /home/user/codex/langchain/.venv/lib64/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /home/user/codex/langchain/.venv/lib64/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/user/codex/langchain/.venv/lib64/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/user/codex/langchain/.venv/lib64/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/user/codex/langchain/.venv/lib64/python3.13/site-packages (from pydantic<3,>=1->langchain==0.1.16) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/user/codex/langchain/.venv/lib64/python3.13/site-packages (from pydantic<3,>=1->langchain==0.1.16) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /home/user/codex/langchain/.venv/lib64/python3.13/site-packages (from pydantic<3,>=1->langchain==0.1.16) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/user/codex/langchain/.venv/lib64/python3.13/site-packages (from pydantic<3,>=1->langchain==0.1.16) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/user/codex/langchain/.venv/lib64/python3.13/site-packages (from requests<3,>=2->langchain==0.1.16) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/user/codex/langchain/.venv/lib64/python3.13/site-packages (from requests<3,>=2->langchain==0.1.16) (2.3.0)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain==0.1.16)\n",
      "  Using cached greenlet-3.3.0-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.16)\n",
      "  Using cached mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Using cached langchain-0.1.16-py3-none-any.whl (817 kB)\n",
      "Downloading aiohttp-3.13.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
      "Using cached langchain_core-0.1.53-py3-none-any.whl (303 kB)\n",
      "Using cached langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\n",
      "Using cached langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
      "Using cached marshmallow-3.26.2-py3-none-any.whl (50 kB)\n",
      "Downloading multidict-6.7.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (254 kB)\n",
      "Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached sqlalchemy-2.0.45-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.3 MB)\n",
      "Using cached tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading yarl-1.22.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.8.0-cp313-cp313-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (234 kB)\n",
      "Using cached greenlet-3.3.0-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (612 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Downloading propcache-0.4.1-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (204 kB)\n",
      "Building wheels for collected packages: numpy\n",
      "  Building wheel for numpy (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for numpy: filename=numpy-1.26.4-cp313-cp313-linux_x86_64.whl size=9183416 sha256=0e48eab33987807a57ee304470935b47f80fb918063d9a01281f68e0afc7b858\n",
      "  Stored in directory: /home/user/.cache/pip/wheels/8b/2d/9f/b6b46373f328e2ef50388915d351ccacbedac929459b5459bf\n",
      "Successfully built numpy\n",
      "Installing collected packages: tenacity, propcache, packaging, numpy, mypy-extensions, multidict, jsonpointer, greenlet, frozenlist, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, requests-toolbelt, marshmallow, jsonpatch, aiosignal, langsmith, dataclasses-json, aiohttp, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
      "\u001b[2K  Attempting uninstall: tenacity\n",
      "\u001b[2K    Found existing installation: tenacity 9.1.2\n",
      "\u001b[2K    Uninstalling tenacity-9.1.2:\n",
      "\u001b[2K      Successfully uninstalled tenacity-9.1.2\n",
      "\u001b[2K  Attempting uninstall: packaging\n",
      "\u001b[2K    Found existing installation: packaging 25.0\n",
      "\u001b[2K    Uninstalling packaging-25.0:\n",
      "\u001b[2K      Successfully uninstalled packaging-25.0\n",
      "\u001b[2K  Attempting uninstall: numpym━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/24\u001b[0m [packaging]\n",
      "\u001b[2K    Found existing installation: numpy 2.4.0━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/24\u001b[0m [packaging]\n",
      "\u001b[2K    Uninstalling numpy-2.4.0:90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/24\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled numpy-2.4.0━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/24\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24/24\u001b[0m [langchain]24\u001b[0m [langchain]community]\n",
      "\u001b[1A\u001b[2KSuccessfully installed SQLAlchemy-2.0.45 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 dataclasses-json-0.6.7 frozenlist-1.8.0 greenlet-3.3.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.1.16 langchain-community-0.0.38 langchain-core-0.1.53 langchain-text-splitters-0.0.2 langsmith-0.1.147 marshmallow-3.26.2 multidict-6.7.0 mypy-extensions-1.1.0 numpy-1.26.4 packaging-23.2 propcache-0.4.1 requests-toolbelt-1.0.0 tenacity-8.5.0 typing-inspect-0.9.0 yarl-1.22.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain==0.1.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84fb644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-2.9.1%2Bcpu-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: filelock in /home/user/codex/langchain/.venv312/lib64/python3.12/site-packages (from torch) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/user/codex/langchain/.venv312/lib64/python3.12/site-packages (from torch) (4.15.0)\n",
      "Collecting setuptools (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/setuptools-70.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/user/codex/langchain/.venv312/lib64/python3.12/site-packages (from torch) (1.14.0)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Using cached networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /home/user/codex/langchain/.venv312/lib64/python3.12/site-packages (from torch) (2025.12.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/user/codex/langchain/.venv312/lib64/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading https://download.pytorch.org/whl/MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
      "Downloading https://download.pytorch.org/whl/cpu/torch-2.9.1%2Bcpu-cp312-cp312-manylinux_2_28_x86_64.whl (184.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.4/184.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hUsing cached networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "Using cached https://download.pytorch.org/whl/jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached https://download.pytorch.org/whl/setuptools-70.2.0-py3-none-any.whl (930 kB)\n",
      "Installing collected packages: setuptools, networkx, MarkupSafe, jinja2, torch\n",
      "Successfully installed MarkupSafe-2.1.5 jinja2-3.1.6 networkx-3.6.1 setuptools-70.2.0 torch-2.9.1+cpu\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"pandas>=2.2.3\"\n",
    "!pip install ibm-watsonx-ai\n",
    "Then manually install other missing dependencies\n",
    "!pip install ibm-watsonx-ai==0.2.6\n",
    "!pip install langchain==0.1.16\n",
    "!pip install langchain-ibm==0.1.4\n",
    "!pip install transformers==4.41.2\n",
    "!pip install huggingface-hub==0.23.4\n",
    "!pip install sentence-transformers==2.5.1\n",
    "!pip install chromadb\n",
    "pip install sentence-transformers\n",
    "!pip install wget==3.2\n",
    "!pip install --upgrade torch --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b498f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ibm-watsonx-ai\n",
      "  Downloading ibm_watsonx_ai-1.4.11-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting requests (from ibm-watsonx-ai)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting httpx<0.29,>=0.27 (from ibm-watsonx-ai)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting urllib3 (from ibm-watsonx-ai)\n",
      "  Using cached urllib3-2.6.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting pandas<2.3.0,>=0.24.2 (from ibm-watsonx-ai)\n",
      "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting certifi (from ibm-watsonx-ai)\n",
      "  Using cached certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting lomond (from ibm-watsonx-ai)\n",
      "  Downloading lomond-0.3.3-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting tabulate (from ibm-watsonx-ai)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: packaging in /home/user/codex/langchain/.venv312/lib64/python3.12/site-packages (from ibm-watsonx-ai) (25.0)\n",
      "Collecting ibm-cos-sdk<2.15.0,>=2.12.0 (from ibm-watsonx-ai)\n",
      "  Downloading ibm_cos_sdk-2.14.3.tar.gz (58 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cachetools (from ibm-watsonx-ai)\n",
      "  Using cached cachetools-6.2.4-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting anyio (from httpx<0.29,>=0.27->ibm-watsonx-ai)\n",
      "  Using cached anyio-4.12.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting httpcore==1.* (from httpx<0.29,>=0.27->ibm-watsonx-ai)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx<0.29,>=0.27->ibm-watsonx-ai)\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<0.29,>=0.27->ibm-watsonx-ai)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting ibm-cos-sdk-core==2.14.3 (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai)\n",
      "  Downloading ibm_cos_sdk_core-2.14.3.tar.gz (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ibm-cos-sdk-s3transfer==2.14.3 (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai)\n",
      "  Downloading ibm_cos_sdk_s3transfer-2.14.3.tar.gz (139 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jmespath<=1.0.1,>=0.10.0 (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.9.0 in /home/user/codex/langchain/.venv312/lib64/python3.12/site-packages (from ibm-cos-sdk-core==2.14.3->ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai) (2.9.0.post0)\n",
      "Collecting numpy>=1.26.0 (from pandas<2.3.0,>=0.24.2->ibm-watsonx-ai)\n",
      "  Downloading numpy-2.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting pytz>=2020.1 (from pandas<2.3.0,>=0.24.2->ibm-watsonx-ai)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas<2.3.0,>=0.24.2->ibm-watsonx-ai)\n",
      "  Using cached tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->ibm-watsonx-ai)\n",
      "  Downloading charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/user/codex/langchain/.venv312/lib64/python3.12/site-packages (from lomond->ibm-watsonx-ai) (1.17.0)\n",
      "Collecting typing_extensions>=4.5 (from anyio->httpx<0.29,>=0.27->ibm-watsonx-ai)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Downloading ibm_watsonx_ai-1.4.11-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "Using cached urllib3-2.6.2-py3-none-any.whl (131 kB)\n",
      "Using cached cachetools-6.2.4-py3-none-any.whl (11 kB)\n",
      "Downloading lomond-0.3.3-py2.py3-none-any.whl (35 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading numpy-2.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "Using cached anyio-4.12.0-py3-none-any.whl (113 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Building wheels for collected packages: ibm-cos-sdk, ibm-cos-sdk-core, ibm-cos-sdk-s3transfer\n",
      "  Building wheel for ibm-cos-sdk (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ibm-cos-sdk: filename=ibm_cos_sdk-2.14.3-py3-none-any.whl size=76930 sha256=192cd0f9cdc1c2901c10880c668905b4aa2269563804170060102e0b2b3b9474\n",
      "  Stored in directory: /home/user/.cache/pip/wheels/cc/2f/6f/125918ad46d280d3bea58edf99f0757888ef6e7999db4b73b7\n",
      "  Building wheel for ibm-cos-sdk-core (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ibm-cos-sdk-core: filename=ibm_cos_sdk_core-2.14.3-py3-none-any.whl size=659959 sha256=a9c7a2de7e694b3bc6c1113276653b176476ce949eeb2e6926c2e78b2f4ec347\n",
      "  Stored in directory: /home/user/.cache/pip/wheels/f1/53/13/7c8fdeebdb847995d8ef349b4f695c595d8d31b30ae2a07ea2\n",
      "  Building wheel for ibm-cos-sdk-s3transfer (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ibm-cos-sdk-s3transfer: filename=ibm_cos_sdk_s3transfer-2.14.3-py3-none-any.whl size=89981 sha256=7b51fa6c30c5aaf939b5a0bfacf9725ed031cb86da6d62229d9f622a2cd70906\n",
      "  Stored in directory: /home/user/.cache/pip/wheels/0c/8b/10/0346c5a955b48b7fca50a8a42de309546b22899b7e8c1da8a5\n",
      "Successfully built ibm-cos-sdk ibm-cos-sdk-core ibm-cos-sdk-s3transfer\n",
      "Installing collected packages: pytz, urllib3, tzdata, typing_extensions, tabulate, numpy, lomond, jmespath, idna, h11, charset_normalizer, certifi, cachetools, requests, pandas, httpcore, anyio, ibm-cos-sdk-core, httpx, ibm-cos-sdk-s3transfer, ibm-cos-sdk, ibm-watsonx-ai\n",
      "Successfully installed anyio-4.12.0 cachetools-6.2.4 certifi-2025.11.12 charset_normalizer-3.4.4 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 ibm-cos-sdk-2.14.3 ibm-cos-sdk-core-2.14.3 ibm-cos-sdk-s3transfer-2.14.3 ibm-watsonx-ai-1.4.11 idna-3.11 jmespath-1.0.1 lomond-0.3.3 numpy-2.4.0 pandas-2.2.3 pytz-2025.2 requests-2.32.5 tabulate-0.9.0 typing_extensions-4.15.0 tzdata-2025.3 urllib3-2.6.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ibm-watsonx-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6494928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5a7f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/codex/langchain/.venv312/lib64/python3.12/site-packages/ibm_watson_machine_learning/foundation_models/extensions/langchain/llm.py:60: WatsonxLLMDeprecationWarning: ibm_watson_machine_learning.foundation_models.extensions.langchain.WatsonxLLM is deprecated and will not be supported in the future. Please import from langchain-ibm instead.\n",
      "To install langchain-ibm run `pip install -U langchain-ibm`.\n",
      "  _raise_watsonxllm_deprecation_warning()\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from ibm_watsonx_ai.foundation_models import Model\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes, DecodingMethods\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "import wget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20fa452",
   "metadata": {},
   "source": [
    "<!-- ##### PreProcessing -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f874cf",
   "metadata": {},
   "source": [
    "<!-- 1. Load Document -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa285088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file downloaded\n"
     ]
    }
   ],
   "source": [
    "filename = 'companyPolicies.txt'\n",
    "url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/6JDbUb_L3egv_eOkouY71A.txt'\n",
    "\n",
    "wget.download(url,filename)\n",
    "print('file downloaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22bf2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\tCode of Conduct\n",
      "\n",
      "Our Code of Conduct outlines the fundamental principles and ethical standards that guide every member of our organization. We are committed to maintaining a workplace that is built on integrity, respect, and accountability.\n",
      "Integrity: We hold ourselves to the highest ethical standards. This means acting honestly and transparently in all our interactions, whether with colleagues, clients, or the broader community. We respect and protect sensitive information, and we avoid conflicts of interest.\n",
      "Respect: We embrace diversity and value each individual's contributions. Discrimination, harassment, or any form of disrespectful behavior is unacceptable. We create an inclusive environment where differences are celebrated and everyone is treated with dignity and courtesy.\n",
      "Accountability: We take responsibility for our actions and decisions. We follow all relevant laws and regulations, and we strive to continuously improve our practices. We report any potential violations of this code and support the investigation of such matters.\n",
      "Safety: We prioritize the safety of our employees, clients, and the communities we serve. We maintain a culture of safety, including reporting any unsafe conditions or practices.\n",
      "Environmental Responsibility: We are committed to minimizing our environmental footprint and promoting sustainable practices.\n",
      "Our Code of Conduct is not just a set of rules; it is the foundation of our organization's culture. We expect all employees to uphold these principles and serve as role models for others, ensuring we maintain our reputation for ethical conduct, integrity, and social responsibility.\n",
      "\n",
      "2.\tRecruitment Policy\n",
      "\n",
      "Our Recruitment Policy reflects our commitment to attracting, selecting, and onboarding the most qualified and diverse candidates to join our organization. We believe that the success of our company relies on the talents, skills, and dedication of our employees.\n",
      "Equal Opportunity: We are an equal opportunity employer and do not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or any other protected status. We actively promote diversity and inclusion.\n",
      "Transparency: We maintain transparency in our recruitment processes. All job vacancies are advertised internally and externally when appropriate. Job descriptions and requirements are clear and accurately represent the role.\n",
      "Selection Criteria: Our selection process is based on the qualifications, experience, and skills necessary for the position. Interviews and assessments are conducted objectively, and decisions are made without bias.\n",
      "Data Privacy: We are committed to protecting the privacy of candidates' personal information and adhere to all relevant data protection laws and regulations.\n",
      "Feedback: Candidates will receive timely and constructive feedback on their application and interview performance.\n",
      "Onboarding: New employees receive comprehensive onboarding to help them integrate into the organization effectively. This includes information on our culture, policies, and expectations.\n",
      "Employee Referrals: We encourage and appreciate employee referrals as they contribute to building a strong and engaged team.\n",
      "Our Recruitment Policy is a foundation for creating a diverse, inclusive, and talented workforce. It ensures that we attract and hire the best candidates who align with our company values and contribute to our continued success. We continuously review and update this policy to reflect evolving best practices in recruitment.\n",
      "\n",
      "3.\tInternet and Email Policy\n",
      "\n",
      "Our Internet and Email Policy is established to guide the responsible and secure use of these essential tools within our organization. We recognize their significance in daily business operations and the importance of adhering to principles that maintain security, productivity, and legal compliance.\n",
      "Acceptable Use: Company-provided internet and email services are primarily meant for job-related tasks. Limited personal use is allowed during non-work hours, provided it doesn't interfere with work responsibilities.\n",
      "Security: Safeguard your login credentials, avoiding the sharing of passwords. Exercise caution with email attachments and links from unknown sources. Promptly report any unusual online activity or potential security breaches.\n",
      "Confidentiality: Reserve email for the transmission of confidential information, trade secrets, and sensitive customer data only when encryption is applied. Exercise discretion when discussing company matters on public forums or social media.\n",
      "Harassment and Inappropriate Content: Internet and email usage must not involve harassment, discrimination, or the distribution of offensive or inappropriate content. Show respect and sensitivity to others in all online communications.\n",
      "Compliance: Ensure compliance with all relevant laws and regulations regarding internet and email usage, including those related to copyright and data protection.\n",
      "Monitoring: The company retains the right to monitor internet and email usage for security and compliance purposes.\n",
      "Consequences: Policy violations may lead to disciplinary measures, including potential termination.\n",
      "Our Internet and Email Policy aims to promote safe, responsible usage of digital communication tools that align with our values and legal obligations. Each employee is expected to understand and follow this policy. Regular reviews ensure its alignment with evolving technology and security standards.\n",
      "\n",
      "4.\tMobile Phone Policy\n",
      "\n",
      "The Mobile Phone Policy sets forth the standards and expectations governing the appropriate and responsible usage of mobile devices in the organization. The purpose of this policy is to ensure that employees utilize mobile phones in a manner consistent with company values and legal compliance.\n",
      "Acceptable Use: Mobile devices are primarily intended for work-related tasks. Limited personal usage is allowed, provided it does not disrupt work obligations.\n",
      "Security: Safeguard your mobile device and access credentials. Exercise caution when downloading apps or clicking links from unfamiliar sources. Promptly report security concerns or suspicious activities related to your mobile device.\n",
      "Confidentiality: Avoid transmitting sensitive company information via unsecured messaging apps or emails. Be discreet when discussing company matters in public spaces.\n",
      "Cost Management: Keep personal phone usage separate from company accounts and reimburse the company for any personal charges on company-issued phones.\n",
      "Compliance: Adhere to all pertinent laws and regulations concerning mobile phone usage, including those related to data protection and privacy.\n",
      "Lost or Stolen Devices: Immediately report any lost or stolen mobile devices to the IT department or your supervisor.\n",
      "Consequences: Non-compliance with this policy may lead to disciplinary actions, including the potential loss of mobile phone privileges.\n",
      "The Mobile Phone Policy is aimed at promoting the responsible and secure use of mobile devices in line with legal and ethical standards. Every employee is expected to comprehend and abide by these guidelines. Regular reviews of the policy ensure its ongoing alignment with evolving technology and security best practices.\n",
      "\n",
      "5.\tSmoking Policy\n",
      "\n",
      "Policy Purpose: The Smoking Policy has been established to provide clear guidance and expectations concerning smoking on company premises. This policy is in place to ensure a safe and healthy environment for all employees, visitors, and the general public.\n",
      "Designated Smoking Areas: Smoking is only permitted in designated smoking areas, as marked by appropriate signage. These areas have been chosen to minimize exposure to secondhand smoke and to maintain the overall cleanliness of the premises.\n",
      "Smoking Restrictions: Smoking inside company buildings, offices, meeting rooms, and other enclosed spaces is strictly prohibited. This includes electronic cigarettes and vaping devices.\n",
      "Compliance with Applicable Laws: All employees and visitors must adhere to relevant federal, state, and local smoking laws and regulations.\n",
      "Disposal of Smoking Materials: Properly dispose of cigarette butts and related materials in designated receptacles. Littering on company premises is prohibited.\n",
      "No Smoking in Company Vehicles: Smoking is not permitted in company vehicles, whether they are owned or leased, to maintain the condition and cleanliness of these vehicles.\n",
      "Enforcement and Consequences: All employees and visitors are expected to adhere to this policy. Non-compliance may lead to appropriate disciplinary action, which could include fines, or, in the case of employees, possible termination of employment.\n",
      "Review of Policy: This policy will be reviewed periodically to ensure its alignment with evolving legal requirements and best practices for maintaining a healthy and safe workplace.\n",
      "We appreciate your cooperation in maintaining a smoke-free and safe environment for all.\n",
      "\n",
      "6.\tDrug and Alcohol Policy\n",
      "\n",
      "Policy Objective: The Drug and Alcohol Policy is established to establish clear expectations and guidelines for the responsible use of drugs and alcohol within the organization. This policy aims to maintain a safe, healthy, and productive workplace.\n",
      "Prohibited Substances: The use, possession, distribution, or sale of illegal drugs or unauthorized controlled substances is strictly prohibited on company premises or during work-related activities. This includes the misuse of prescription drugs.\n",
      "Alcohol Consumption: The consumption of alcoholic beverages is not allowed during work hours, on company property, or while performing company-related duties. Exception may be made for company-sanctioned events.\n",
      "Impairment: Employees are expected to perform their job duties without impairment from drugs or alcohol. The use of substances that could impair job performance or pose a safety risk is prohibited.\n",
      "Testing and Searches: The organization reserves the right to conduct drug and alcohol testing as per applicable laws and regulations. Employees may be subject to testing in cases of reasonable suspicion, post-accident, or as part of routine workplace safety measures.\n",
      "Reporting: Employees should report any concerns related to drug or alcohol misuse by themselves or their colleagues, as well as safety concerns arising from such misuse.\n",
      "Treatment and Assistance: Employees with substance abuse issues are encouraged to seek help. The organization is committed to providing support, resources, and information to assist those seeking treatment.\n",
      "Consequences: Violation of this policy may result in disciplinary actions, up to and including termination of employment. Legal action may also be pursued when necessary.\n",
      "Policy Review: This policy will undergo periodic review to ensure its continued relevance and compliance with evolving legal requirements and best practices for a safe and productive work environment.\n",
      "Your adherence to this policy is appreciated as it helps to maintain a safe and drug-free workplace for all.\n",
      "\n",
      "7.\tHealth and Safety Policy\n",
      "\n",
      "Our commitment to health and safety is paramount. We prioritize the well-being of our employees, customers, and the public. We diligently comply with all relevant health and safety laws and regulations. Our objective is to maintain a workplace free from hazards, preventing accidents, injuries, and illnesses. Every individual within our organization is responsible for upholding these standards. We regularly assess and improve our safety measures, provide adequate training, and encourage open communication regarding safety concerns. Through collective dedication, we aim to ensure a safe, healthy, and secure environment for all. Your cooperation is essential in achieving this common goal.\n",
      "\n",
      "8.\tAnti-discrimination and Harassment Policy\n",
      "\n",
      "The Anti-Discrimination and Harassment Policy is a testament to the commitment of this organization in fostering a workplace that is free from discrimination, harassment, and any form of unlawful bias. This policy applies to every individual within the organization, including employees, contractors, visitors, and clients.\n",
      "Non-Discrimination: This organization strictly prohibits discrimination based on race, color, religion, gender, national origin, age, disability, sexual orientation, or any other legally protected characteristic in all aspects of employment, including recruitment, hiring, compensation, benefits, promotions, and terminations.\n",
      "Harassment: Harassment in any form, whether based on the aforementioned characteristics or any other protected status, is unacceptable. This encompasses unwelcome advances, offensive jokes, slurs, and other verbal or physical conduct that creates a hostile or intimidating work environment.\n",
      "Reporting: Individuals who experience or witness any form of discrimination or harassment are encouraged to promptly report the incident to their supervisor, manager, or the designated HR representative. The organization is committed to a timely and confidential investigation of such complaints.\n",
      "Consequences: Violation of this policy may result in disciplinary action, including termination of employment. The organization is committed to taking appropriate action against any individual found to be in violation of this policy.\n",
      "Review and Update: This policy is subject to regular review and update to remain aligned with evolving legal requirements and best practices in preventing discrimination and harassment. This organization considers it a collective responsibility to ensure a workplace free from discrimination and harassment, and it is essential that every individual within the organization plays their part in upholding these principles.\n",
      "\n",
      "9.\tDiscipline and Termination Policy\n",
      "\n",
      "The Discipline and Termination Policy underscores the organization's commitment to maintaining a productive, ethical, and respectful work environment. This policy applies to all personnel, including employees, contractors, and temporary staff.\n",
      "Performance and Conduct Expectations: Employees are expected to meet performance standards and adhere to conduct guidelines. The organization will provide clear expectations, feedback, and opportunities for improvement when performance or conduct issues arise.\n",
      "Disciplinary Actions: When necessary, disciplinary actions will be taken, which may include verbal warnings, written warnings, suspension, or other appropriate measures. Disciplinary actions are designed to address issues constructively and maintain performance standards.\n",
      "Termination: In situations where an employee's performance or conduct issues persist, the organization may resort to termination. Termination may also occur for reasons such as redundancy, violation of policies, or restructuring.\n",
      "Termination Procedure: The organization will follow appropriate procedures, ensuring fairness and adherence to legal requirements during the termination process. Employees may be eligible for notice periods, severance pay, or other benefits as per employment agreements and applicable laws.\n",
      "Exit Process: The organization will conduct an exit process to ensure a smooth transition for departing employees, including the return of company property, final pay, and cancellation of access and benefits.\n",
      "This policy serves as a framework for handling discipline and termination. The organization recognizes the importance of fairness and consistency in these processes, and decisions will be made after careful consideration. Every employee is expected to understand and adhere to this policy, contributing to a respectful and productive workplace. Regular reviews will ensure its alignment with evolving legal requirements and best practices.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(filename,'r') as file:\n",
    "    contents = file.read()\n",
    "    print(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13279c16",
   "metadata": {},
   "source": [
    "<!-- 2. Split Documents into chunks -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d647393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1624, which is longer than the specified 1000\n",
      "Created a chunk of size 1885, which is longer than the specified 1000\n",
      "Created a chunk of size 1903, which is longer than the specified 1000\n",
      "Created a chunk of size 1729, which is longer than the specified 1000\n",
      "Created a chunk of size 1678, which is longer than the specified 1000\n",
      "Created a chunk of size 2032, which is longer than the specified 1000\n",
      "Created a chunk of size 1894, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "loader = TextLoader(filename)\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000,chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b319c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.2.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /home/user/codex/langchain/.venv312/lib64/python3.12/site-packages (from sentence-transformers) (4.41.2)\n",
      "Requirement already satisfied: tqdm in /home/user/codex/langchain/.venv312/lib64/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/user/codex/langchain/.venv312/lib64/python3.12/site-packages (from sentence-transformers) (2.9.1+cpu)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Using cached scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Using cached scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/user/codex/langchain/.venv312/lib64/python3.12/site-packages (from sentence-transformers) (0.23.4)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/user/codex/langchain/.venv312/lib64/python3.12/site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /home/user/codex/langchain/.venv312/lib64/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/user/codex/langchain/.venv312/lib64/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/user/codex/langchain/.venv312/lib64/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/user/codex/langchain/.venv312/lib64/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: requests in /home/user/codex/langchain/.venv312/lib64/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: setuptools in /home/user/codex/langchain/.venv312/lib64/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (70.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/user/codex/langchain/.venv312/lib64/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/user/codex/langchain/.venv312/lib64/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /home/user/codex/langchain/.venv312/lib64/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/user/codex/langchain/.venv312/lib64/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/user/codex/langchain/.venv312/lib64/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/user/codex/langchain/.venv312/lib64/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/user/codex/langchain/.venv312/lib64/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Collecting joblib>=1.3.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/user/codex/langchain/.venv312/lib64/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/user/codex/langchain/.venv312/lib64/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/user/codex/langchain/.venv312/lib64/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user/codex/langchain/.venv312/lib64/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/user/codex/langchain/.venv312/lib64/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user/codex/langchain/.venv312/lib64/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.11.12)\n",
      "Downloading sentence_transformers-5.2.0-py3-none-any.whl (493 kB)\n",
      "Downloading scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn, sentence-transformers\n",
      "Successfully installed joblib-1.5.3 scikit-learn-1.8.0 scipy-1.16.3 sentence-transformers-5.2.0 threadpoolctl-3.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e91043ef",
   "metadata": {},
   "source": [
    "<!-- 3. Embedding and storing -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c344989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/codex/langchain/.venv312/lib64/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document ingested\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings()\n",
    "docsearch = Chroma.from_documents(texts,embeddings)\n",
    "print('document ingested')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aacfac",
   "metadata": {},
   "source": [
    "<!-- ##### LLM model construction -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71322993",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'ibm/granite-3-3-8b-instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a59883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    GenParams.DECODING_METHOD : DecodingMethods.GREEDY,\n",
    "    GenParams.MIN_NEW_TOKENS: 130,\n",
    "    GenParams.MAX_NEW_TOKENS: 256,\n",
    "    GenParams.TEMPERATURE:0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20ef018",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = {\n",
    "    'url': \"https://us-south.ml.cloud.ibm.com\",\n",
    "    # \"api_key\":\"enter key\",\n",
    "}\n",
    "\n",
    "project_id = \"skills-network\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c59021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    model_id = model_id,\n",
    "    params = parameters,\n",
    "    credentials = credentials,\n",
    "    project_id = project_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d93754",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = WatsonxLLM(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af4d622",
   "metadata": {},
   "source": [
    "<!-- ##### Integrating LangChain -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf84df8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever(),\n",
    "    return_source_documents=False\n",
    "\n",
    ")\n",
    "\n",
    "query = \"what is mobile Policy\"\n",
    "qa.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e31323",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever,\n",
    "    return_source_documents=False\n",
    ")\n",
    "\n",
    "query = \"Can you summarize document ?\"\n",
    "qa.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26efa4e9",
   "metadata": {},
   "source": [
    "<!-- ##### Prompt Template -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d4b993",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\" Use the information from document to answer the questions \n",
    " at the end. If you don't know the answer, just say that you don't know, \n",
    " definately do not try to make up an answer.\n",
    " \n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\",\"question\"]\n",
    ")\n",
    "\n",
    "chain_type_kwargs = {\"prompt\":PROMPT}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6a03c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever(),\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    "    return_source_documents=False\n",
    ")\n",
    "\n",
    "query = \"Can I eat in company vehicles?\"\n",
    "qa.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7136eebc",
   "metadata": {},
   "source": [
    "<!-- ##### Give Memory to Conversations -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76debe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_message=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8272edf5",
   "metadata": {},
   "source": [
    "<!-- create ConversationalRetrievalChain to retrieve Information and talk with LLM -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee541b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever(),\n",
    "    get_chat_history=lambda h:h,\n",
    "    return_source_documents=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9545ed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195bdf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is mobile policy?\"\n",
    "result = qa.invoke({\"question\":query},{\"chat_history\":history})\n",
    "print(result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b36f0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.append((query,result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88deede6",
   "metadata": {},
   "outputs": [],
   "source": [
    "quey=\"List points in it?\"\n",
    "result = qa.invoke({\"question\":query},{\"chat_history\":history})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a216d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.append((query,result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a1e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is the aim of it?\"\n",
    "result = qa.invoke({\"question\":query},{\"chat_history\":history})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca635316",
   "metadata": {},
   "source": [
    "<!-- ##### Make Agent -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7726fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa():\n",
    "    memory = ConversationBufferMemory(memory_key = \"chat_history\", return_message = True)\n",
    "    qa = ConversationalRetrievalChain.from_llm(llm=llama_3_llm, \n",
    "                                               chain_type=\"stuff\", \n",
    "                                               retriever=docsearch.as_retriever(), \n",
    "                                               memory = memory, \n",
    "                                               get_chat_history=lambda h : h, \n",
    "                                               return_source_documents=False)\n",
    "    history = []\n",
    "\n",
    "    while True:\n",
    "        query = input(\"Question: \")\n",
    "        if query.lower() in [\"quit\",\"exit\",\"bye\"]:\n",
    "            print(\"Answer: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        result = qa({\"question\": query},{\"chat_history\":history})\n",
    "        history.append((query,result['answer']))\n",
    "\n",
    "        print(\"Answer: \",result[\"answer\"])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33026960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Agent\n",
    "qa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99c005e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
